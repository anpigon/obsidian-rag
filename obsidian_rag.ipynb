{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.directory import DirectoryLoader\n",
    "from langchain_community.document_loaders.markdown import UnstructuredMarkdownLoader\n",
    "from langchain_community.document_loaders.text import TextLoader\n",
    "from langchain_community.document_loaders.unstructured import UnstructuredFileLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    \"/Users/anpigon/ObsidianVault/Documents\",\n",
    "    glob=\"**/*.md\",  # .md 확장자를 가진 파일만 대상으로 함\n",
    "    recursive=True,  # 하위 디렉토리까지 모두 탐색\n",
    "    show_progress=True,  # 진행 상태를 표시\n",
    "    loader_cls=TextLoader, \n",
    ")\n",
    "documents = loader.load()\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encountered non-yaml frontmatter\n",
      "Encountered non-yaml frontmatter\n",
      "Encountered non-yaml frontmatter\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'Dashboard.md', 'path': '/Users/anpigon/ObsidianVault/Documents/Dashboard.md', 'created': '2024-05-21 10:11:24', 'last_modified': 1721313244.8919904, 'last_accessed': 1721391469.7960355, 'date': '2024-05-21 10:11:24', 'updated': '2024-07-18 11:34:03', 'cssclasses': \"['wide']\"}, page_content='\\n### 프로젝트\\n```dataview\\nTABLE WITHOUT ID \\n\\t\"[[\"+ file.path + \"|\"+ default(file.aliases[0], file.name) +\"]]\" AS 제목, \\n\\tstatus as 상태 \\nWHERE type = \"project\"\\nAND !contains(status, \"중단\")\\nAND !contains(status, \"완료\")\\nSORT status ASC, category DESC\\n```\\n\\n### 최근 메모\\n```dataview\\nTABLE WITHOUT ID \\n\\t\"[[\"+ file.path + \"|\"+ default(file.aliases[0], file.name) +\"]]\" AS 제목,\\n\\tdateformat(file.ctime,\"yyyy-MM-dd\") as 날짜\\nFROM \"5 Inbox\"\\nWHERE file.name != file.folder\\nSORT file.ctime DESC\\nLIMIT 10\\n```\\n\\n### 최근 스크랩\\n```dataview\\nTABLE WITHOUT ID \\n\\t\"[[\"+ file.path + \"|\"+ default(file.aliases[0], file.name) +\"]]\" AS 제목,\\n\\tchoice(contains(title, \"GeekNews\"), \"`GeekNews`,\", \"\") + join(map(filter(list(tags), (x) => !!x), (x) => \"`\"+x+\"`\"), \", \") as 태그,\\n\\tdateformat(file.ctime,\"yyyy-MM-dd\") as 날짜\\nFROM #스크랩\\nSORT file.ctime DESC\\nLIMIT 20\\n```\\n')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "from langchain_community.document_loaders.obsidian import ObsidianLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "class MyObsidianLoader(ObsidianLoader):\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        paths = list(Path(self.file_path).glob(\"**/*.md\"))\n",
    "        for path in paths:\n",
    "            with open(path, encoding=self.encoding) as f:\n",
    "                text = f.read()\n",
    "\n",
    "            try:\n",
    "                front_matter = self._parse_front_matter(text)\n",
    "                tags = self._parse_document_tags(text)\n",
    "                dataview_fields = self._parse_dataview_fields(text)\n",
    "                text = self._remove_front_matter(text)\n",
    "                metadata = {\n",
    "                    \"source\": str(path.name),\n",
    "                    \"path\": str(path),\n",
    "                    \"created\": path.stat().st_ctime,\n",
    "                    \"last_modified\": path.stat().st_mtime,\n",
    "                    \"last_accessed\": path.stat().st_atime,\n",
    "                    **self._to_langchain_compatible_metadata(front_matter),\n",
    "                    **dataview_fields,\n",
    "                }\n",
    "\n",
    "                if tags or front_matter.get(\"tags\"):\n",
    "                    metadata[\"tags\"] = \",\".join(\n",
    "                        tags | set(front_matter.get(\"tags\", []) or [])\n",
    "                    )\n",
    "            except:\n",
    "                metadata = {\n",
    "                    \"source\": str(path.name),\n",
    "                    \"path\": str(path),\n",
    "                    \"created\": path.stat().st_ctime,\n",
    "                    \"last_modified\": path.stat().st_mtime,\n",
    "                    \"last_accessed\": path.stat().st_atime,\n",
    "                }\n",
    "\n",
    "            yield Document(page_content=text, metadata=metadata)\n",
    "\n",
    "\n",
    "loader = MyObsidianLoader(\n",
    "    \"/Users/anpigon/ObsidianVault/Documents\", encoding=\"utf-8\", collect_metadata=True\n",
    ")\n",
    "documents = loader.load()\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6631"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'Dashboard.md', 'path': '/Users/anpigon/ObsidianVault/Documents/Dashboard.md', 'created': '2024-05-21 10:11:24', 'last_modified': 1721313244.8919904, 'last_accessed': 1721391469.7960355, 'date': '2024-05-21 10:11:24', 'updated': '2024-07-18 11:34:03', 'cssclasses': \"['wide']\"}, page_content='### 프로젝트\\n```dataview\\nTABLE WITHOUT ID \\n\\t\"[[\"+ file.path + \"|\"+ default(file.aliases[0], file.name) +\"]]\" AS 제목, \\n\\tstatus as 상태 \\nWHERE type = \"project\"\\nAND !contains(status, \"중단\")\\nAND !contains(status, \"완료\")\\nSORT status ASC, category DESC\\n```\\n\\n### 최근 메모\\n```dataview\\nTABLE WITHOUT ID \\n\\t\"[[\"+ file.path + \"|\"+ default(file.aliases[0], file.name) +\"]]\" AS 제목,\\n\\tdateformat(file.ctime,\"yyyy-MM-dd\") as 날짜\\nFROM \"5 Inbox\"\\nWHERE file.name != file.folder\\nSORT file.ctime DESC\\nLIMIT 10\\n```\\n\\n### 최근 스크랩\\n```dataview\\nTABLE WITHOUT ID \\n\\t\"[[\"+ file.path + \"|\"+ default(file.aliases[0], file.name) +\"]]\" AS 제목,\\n\\tchoice(contains(title, \"GeekNews\"), \"`GeekNews`,\", \"\") + join(map(filter(list(tags), (x) => !!x), (x) => \"`\"+x+\"`\"), \", \") as 태그,\\n\\tdateformat(file.ctime,\"yyyy-MM-dd\") as 날짜\\nFROM #스크랩\\nSORT file.ctime DESC\\nLIMIT 20\\n```')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import Language, RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    chunk_size=1024, chunk_overlap=24, language=Language.MARKDOWN\n",
    ")\n",
    "splitted_documents = text_splitter.split_documents(documents)\n",
    "splitted_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33752"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splitted_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|██████████| 30/30 [01:11<00:00,  2.37s/it]\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel, FlagModel\n",
    "\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "bge_embeddings = BGEM3FlagModel(\n",
    "    model_name, use_fp16=True\n",
    ")  # use_fp16을 True로 설정하면 약간의 성능 저하와 함께 계산 속도가 빨라집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings, HuggingFaceEmbeddings\n",
    "\n",
    "root_path = Path.cwd()\n",
    "store = LocalFileStore(root_path / \".cached_embeddings\")\n",
    "\n",
    "model_name = \"BAAI/bge-m3\" # \"intfloat/multilingual-e5-large-instruct\"\n",
    "model_kwargs = {\"device\": \"mps\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "underlying_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    underlying_embeddings=underlying_embeddings, document_embedding_cache=store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    splitted_documents,\n",
    "    cached_embeddings,\n",
    "    persist_directory=\"vectorstore\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " vectorstore_retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.retrievers import KiwiBM25Retriever\n",
    "\n",
    "bm25_retriever = KiwiBM25Retriever.from_texts(splitted_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, vectorstore_retriever],\n",
    "    weights=[0.4, 0.6],\n",
    "    search_type=\"mmr\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.1, max_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=ensemble_retriever,\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"당신은 마크다운으로 작성한 노트에 대한 사용자의 의문점이나 질문에 도움을 주는 것이 주된 목적인 어시스턴트입니다. 제공된 CONTEXT를 바탕으로 답변을 작성하세요.\n",
    "\n",
    "다음 지침에 따라 질문에 대한 답변을 생성하세요.\n",
    "질문: {input}\n",
    "\n",
    "검색된 다음 컨텍스트 스니펫을 사용해 질문에 답하세요:\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": multi_query_retriever,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Optionally, specify your own session_state key for storing messages\n",
    "msgs = StreamlitChatMessageHistory(key=\"chat_messages\")\n",
    "\n",
    "chain = (\n",
    "    qa_chain\n",
    "    | ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "    | ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    ")\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: msgs,  # Always return the instance created earlier\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "if len(msgs.messages) == 0:\n",
    "    msgs.add_ai_message(\"How can I help you?\")\n",
    "\n",
    "for msg in msgs.messages:\n",
    "    st.chat_message(msg.type).write(msg.content)\n",
    "\n",
    "if prompt := st.chat_input():\n",
    "    st.chat_message(\"human\").write(prompt)\n",
    "\n",
    "    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n",
    "    config = {\"configurable\": {\"session_id\": \"any\"}}\n",
    "    response = chain_with_history.invoke({\"question\": prompt}, config)\n",
    "    st.chat_message(\"ai\").write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'context', 'question'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template='You are an assistant whose primary purpose is to help with questions or inquiries about notes written in Markdown. Base your answer on the provided CONTEXT and the chat history.\\n\\nUse the following context snippets to answer the question:\\n<CONTEXT>\\n{context}\\n</CONTEXT>')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts.chat import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = \"\"\"You are an assistant whose primary purpose is to help with questions or inquiries about notes written in Markdown. Base your answer on the provided CONTEXT and the chat history.\n",
    "\n",
    "Use the following context snippets to answer the question:\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", prompt_template),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obsidian-rag-qjkF7Szi-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
